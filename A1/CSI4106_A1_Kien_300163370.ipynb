{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**CSI 4106 Introduction to Artificial Intelligence** <br/>\n",
        "*Assignment 1: Data Preparation*\n",
        "\n",
        "# Identification\n",
        "\n",
        "Name: <br/>\n",
        "Student Number:\n",
        "\n",
        "# Exploratory Analysis\n",
        "\n",
        "## Import important libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Your code goes here\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Read Dataset\n",
        "\n",
        "As outlined in the project description, it should be possible for the correctors to ecute your notebook without requiring any downloads.\n",
        "\n",
        "To facilitate access to the dataset without the need for downloads, use the data ovided in the public GitHub repository and provide a link to the raw version of the taset.\n",
        "\n",
        "The link to the raw version is as follows:\n",
        "\n",
        "*https://raw.githubusercontent.com/GITHUB_USERNAME/REPOSITORY_NAME/main/DATASETNAME.v*\n",
        "\n",
        "For example:\n",
        "\n",
        "[https://github.com/turcotte/csi4106-f24/blob/main/assignments-data/a1/01/glass.csv]ttps://github.com/turcotte/csi4106-f24/blob/main/assignments-data/a1/01/glass.csv)\n",
        "\n",
        "Now provide the link to YOUR dataset and read the dataset:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Reading dataset '01'\n",
            "Reading dataset '02'\n",
            "Reading dataset '03'\n",
            "Reading dataset '04'\n",
            "Reading dataset '05'\n",
            "Reading dataset '06'\n",
            "Reading dataset '07'\n"
          ]
        }
      ],
      "source": [
        "# Dataset URLs uploaded to my own repo\n",
        "datasets_urls: dict[str, list[str]] = {\n",
        "    \"01\": [\"https://raw.githubusercontent.com/kienmarkdo/CSI4106-AI/main/A1/01_glass/glass.csv\"],\n",
        "    \"02\": [\"https://raw.githubusercontent.com/kienmarkdo/CSI4106-AI/main/A1/02_dermatology/dermatology_database_1.csv\"],\n",
        "    \"03\": [\"https://raw.githubusercontent.com/kienmarkdo/CSI4106-AI/main/A1/03_maternal/Maternal%20Health%20Risk%20Data%20Set.csv\"],\n",
        "    \"04\": [\n",
        "        \"https://raw.githubusercontent.com/kienmarkdo/CSI4106-AI/main/A1/04_car/car.c45-names\",\n",
        "        \"https://raw.githubusercontent.com/kienmarkdo/CSI4106-AI/main/A1/04_car/car.data\",\n",
        "        # \"https://raw.githubusercontent.com/kienmarkdo/CSI4106-AI/main/A1/04_car/car.names\" # this is documentation, not data\n",
        "    ],\n",
        "    \"05\": [\"https://raw.githubusercontent.com/kienmarkdo/CSI4106-AI/main/A1/05_wine/WineQT.csv\"],\n",
        "    \"06\": [\n",
        "        \"https://raw.githubusercontent.com/kienmarkdo/CSI4106-AI/main/A1/06_personalities/16P.csv\",\n",
        "        # \"https://raw.githubusercontent.com/kienmarkdo/CSI4106-AI/main/A1/06_personalities/16p-Mapping.txt\"\n",
        "    ],\n",
        "    \"07\": [\n",
        "        \"https://raw.githubusercontent.com/kienmarkdo/CSI4106-AI/main/A1/07_credit_scores/test.csv\",\n",
        "        \"https://raw.githubusercontent.com/kienmarkdo/CSI4106-AI/main/A1/07_credit_scores/train.csv\"\n",
        "    ],\n",
        "}\n",
        "\n",
        "# Dataframes. Datasets will be read into these dataframes.\n",
        "dataframes: dict[str, list[str]] = {}\n",
        "\n",
        "# Read each dataset and store it in the dataframes list.\n",
        "for topic, urls in datasets_urls.items():\n",
        "    dataframes[topic] = []\n",
        "    print(f\"Reading dataset '{topic}'\")\n",
        "\n",
        "    for url in urls:        \n",
        "        try:\n",
        "            if topic == \"04\":\n",
        "                # uses c45 formatting, where multiple files are used to provide info about a dataset\n",
        "                if url == \"https://raw.githubusercontent.com/kienmarkdo/CSI4106-AI/main/A1/04_car/car.data\":\n",
        "                    # Attribute names based on the car.c45-names file\n",
        "                    column_names = [\"buying\", \"maint\", \"doors\", \"persons\", \"lug_boot\", \"safety\", \"class\"]\n",
        "\n",
        "                    dataset = pd.read_csv(url, names=column_names)\n",
        "                    dataframes[topic].append(dataset)\n",
        "                else:\n",
        "                    continue\n",
        "            if topic == \"06\":  # uses 'cp1252' encoding, not 'utf8':\n",
        "                dataset = pd.read_csv(url, encoding=\"cp1252\", low_memory=False)\n",
        "                dataframes[topic].append(dataset)\n",
        "            else:\n",
        "                dataset = pd.read_csv(url, low_memory=False)\n",
        "                dataframes[topic].append(dataset)\n",
        "        except Exception as e:\n",
        "            print(f\"Failed to read dataset from {url}: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Guidelines\n",
        "\n",
        "The following are the questions for Assignment 1. Under each question, we have provided an initial code cell. You are encouraged to add additional code cells to maintain logical separation of your code. For instance, place the definition of a function in one cell and its execution in a subsequent cell. This approach will help preserve clarity and enhance readability by avoiding the inclusion of excessive code within a single cell.\n",
        "\n",
        "1. **Analysis of Missing Values**: Examine the datasets to identify and assess ssing values in various attributes. Missing values may be represented by symbols ch as '?', empty strings, or other placeholders.\n",
        "\n",
        "    1.1 In the list of options, what are the datasets that contain missing values? ecifically, which attribute or attributes has missing values?\n",
        "\n",
        "    1.2 Describe the methodology used for this investigation, and provide the rresponding code.\n",
        "\n",
        "    1.1 Data imputation involves replacing missing or incomplete data with substituted values to preserve the dataset's integrity for subsequent analysis. Propose imputation strategies for each attribute with missing values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Datasets with missing values:\n",
            "\n",
            "Topic_02_Dataset_1:\n",
            "age    8\n",
            "dtype: int64\n",
            "\n",
            "Topic_07_Dataset_1:\n",
            "Name                       5015\n",
            "Occupation                 3438\n",
            "Monthly_Inhand_Salary      7498\n",
            "Type_of_Loan               5704\n",
            "Num_of_Delayed_Payment     3498\n",
            "Num_Credit_Inquiries       1035\n",
            "Credit_History_Age         4470\n",
            "Amount_invested_monthly    2271\n",
            "Monthly_Balance             562\n",
            "dtype: int64\n",
            "\n",
            "Topic_07_Dataset_2:\n",
            "Name                        9985\n",
            "Occupation                  7062\n",
            "Monthly_Inhand_Salary      15002\n",
            "Type_of_Loan               11408\n",
            "Num_of_Delayed_Payment      7002\n",
            "Num_Credit_Inquiries        1965\n",
            "Credit_History_Age          9030\n",
            "Amount_invested_monthly     4479\n",
            "Monthly_Balance             1200\n",
            "dtype: int64\n",
            "\n",
            "=========================\n"
          ]
        }
      ],
      "source": [
        "# 1.1\n",
        "# Define missing value symbols to ensure any occurence of these symbols is automatically converted to NaN when reading the dataset\n",
        "missing_value_placeholders: list[str] = [\"?\", \"\", \"N/A\", \"null\", \"NULL\",\"_______\"]\n",
        "\n",
        "# Dictionary to store results of missing values\n",
        "missing_values_report: dict = {}\n",
        "\n",
        "# Check each dataset in the list of dataframes\n",
        "for topic, dataset_list in dataframes.items():\n",
        "    for i, dataset in enumerate(dataset_list):\n",
        "        # Use na_values to specify what should be treated as missing\n",
        "        dataset = dataset.replace(missing_value_placeholders, pd.NA)  # Replace placeholders with NaN\n",
        "\n",
        "        # Check for missing values in each column\n",
        "        missing_values = dataset.isnull().sum()  # isnull() detects missing values, sum() tells us how many missing values are in a column\n",
        "        \n",
        "        # Check if any column has missing values\n",
        "        missing_columns = missing_values[missing_values > 0]\n",
        "\n",
        "        if not missing_columns.empty:\n",
        "            # Report which columns have missing values and how many missing values\n",
        "            missing_values_report[f\"Topic_{topic}_Dataset_{i+1}\"] = missing_columns\n",
        "\n",
        "# Print the report\n",
        "if missing_values_report:\n",
        "    print(\"Datasets with missing values:\\n\")\n",
        "    for dataset_name, columns in missing_values_report.items():\n",
        "        print(f\"{dataset_name}:\")\n",
        "        print(columns)\n",
        "        print()\n",
        "    print(\"=========================\")\n",
        "else:\n",
        "    print(\"No missing values found in any dataset.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**1. Explanation**\n",
        "\n",
        "**1.1.**\n",
        "\n",
        "The code above found that dataset 07 has missing values.\n",
        "A quick manual verification shows that it is true.\n",
        "\n",
        "In `test.csv`, we have a row that has a missing `Name` value.\n",
        "- Example: `0x2bf1,CUS_0x83a4,December,,46,167-34-4316,Writer,69577.92,5882.16,...`\n",
        "\n",
        "In `dermatology_database_1.csv`, the `age` attribute is missing a value in some rows too.\n",
        "- Example: `2,2,1,0,0,0,0,0,1,0,1,0,0,2,0,0,2,1,2,2,1,2,0,1,0,0,0,0,0,0,0,0,0,?,1`\n",
        "\n",
        "Other manual checking also shows us that columns like `SSN` also have corrupted data, but are more difficult to detect using automation, since the value is not missing, it's just corrupted.\n",
        "- Example: `0x2bc0,CUS_0x768c,November,Margaret Chadbournu,25,#F%$D@*&8,_______,53908.41_,...`\n",
        "- Here, `SSN` is `#F%$D@*&8`\n",
        "\n",
        "**1.2.**\n",
        "\n",
        "We automate the work of finding missing values in the 7 datasets by iterating through every value of every column of every dataset, then count the number of missing values, defined as any symbol in the following list `[\"?\", \"\", \"N/A\", \"null\", \"NULL\",\"_______\"]`.\n",
        "\n",
        "**1.3.**\n",
        "\n",
        "There are various ways to deal with corrupt data. For numerical attributes, we can replace missing values with the mean, median, or mode of the available data. For categorical attributes, we can replace missing values with the most frequent categories (mode), or simply put \"Unknown\".\n",
        "\n",
        "We can also use machine learning techniques to \"predict\" what those missing values could likely be. For isntance, we can apply linear regression to numerical data to predict missing values based on other features."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "2. **Select and familiarize yourself with a classification task:** Choose one of e provided datasets for further investigation. It is advisable to select a dataset ntaining a sufficiently large number of examples, ideally around 1,000, to ensure bust results when applying machine learning algorithms in the subsequent assignment.\n",
        "\n",
        "    2.1 What is the objective of the task? Is it intended for a specific plication? Do you possess expertise in this particular domain of application?\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Your code goes here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "3. **Attribute Analysis**: \n",
        "\n",
        "    3.1 Determine which attributes lack informativeness and should be excluded to prove the effectiveness of the machine learning analysis. If all features are emed relevant, explicitly state this conclusion.\n",
        "\n",
        "    3.2 Examine the distribution of each attribute (column) within the dataset. Utilize histograms or boxplots to visualize the distributions, identifying any underlying patterns or outliers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Your code goes here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "4. **Class Distribution Analysis**: Investigate the distribution of class labels within the dataset. Employ bar plots to visualize the frequency of instances for each class, and assess whether the dataset is balanced or imbalanced."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Your code goes here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "5. **Preprocessing**: \n",
        "\n",
        "    5.1 For numerical features, determine the best transformation to use. Indicate e transformation that seems appropriate and why. Include the code illustrating how  apply the transformation. For at least one attribute, show the distribution before d after the transformation. See [Preprocessing data](https://scikit-learn.org/able/modules/preprocessing.html).\n",
        "\n",
        "    5.2 For categorical features, show how to apply [one-hot encoding](https://ikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html).  your dataset does not have categorical data, show how to apply the one-hot encoder  the label (target variable)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Your code goes here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "6. **Training and target data**: Set the Python variable `X` to designate the data and `y` to designate the target class. Make sure to select only the informative features."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Your code goes here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "7. **Training and test sets**: Split the dataset into training and testing sets. Reserve 20% of data for testing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Your code goes here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "--------------------------------------------------------------------------\n",
        "\n",
        "# References\n",
        "\n",
        "Make sure you provide references to ALL sources used (articles, code, algorithms).\n",
        "\n",
        "## AI transcript\n",
        "**Hint:** To share a link to your colab notebook, click on \"share\" on the top right. Then, under *General access* , change *Restricted* to \"Anyone with the link\"."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
